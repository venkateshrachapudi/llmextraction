{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# LLM PDF Field Extraction \u2014 Stdlib Only (Fully Commented & Runnable)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# LLM PDF Field Extraction \u2014 Stdlib Only (Fully Commented & Runnable)\n# -------------------------------------------------------------------\n# End-to-end pipeline to extract structured fields from PDFs without extra installs.\n# Uses a best-effort stdlib PDF extractor + heuristics + a mock LLM (swap with a real one).\n#\n# How to run:\n#   1) Place your PDF at /mnt/data/your_file.pdf (or update PDF_PATH below).\n#   2) Run all cells. A JSON result will be saved next to the PDF.\n#\n# NOTE: This extractor is intentionally simple and may not handle all PDFs.\n#       For robustness in production, use a mature PDF library and OCR for scans.\n\nimport os\nimport re\nimport zlib\nimport json\nimport difflib\nimport logging\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional, Tuple\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n\n# -------------------------\n# 1) Schema configuration\n# -------------------------\n\nSchema = List[Dict[str, Any]]\n\nFIELD_SCHEMA: Schema = [\n    # Canonical field + aliases + regex/type hints\n    {\"name\":\"Permit Number\",\"aliases\":[\"Permit #\",\"Permit Num\",\"PermitNumber\",\"Permit No\",\"Permit ID\",\"Permit No.\",\"Permit Nbr\"],\"type\":\"string\",\"regex\":r\"([A-Z0-9-]{3,})\"},\n    {\"name\":\"Issue Date\",\"aliases\":[\"Issued On\",\"Date of Issue\",\"Issuance Date\",\"Date\"],\"type\":\"date\",\"regex\":r\"((?:\\d{4}-\\d{2}-\\d{2})|(?:\\d{2}/\\d{2}/\\d{4})|(?:\\d{2}-\\d{2}-\\d{4}))\"},\n    {\"name\":\"Applicant Name\",\"aliases\":[\"Owner\",\"Applicant\",\"Requestor\",\"Name of Applicant\"],\"type\":\"string\",\"regex\":r\"([A-Za-z ,.'-]{2,})\"},\n    {\"name\":\"Site Address\",\"aliases\":[\"Project Address\",\"Property Address\",\"Location\",\"Address\"],\"type\":\"string\",\"regex\":r\"([0-9A-Za-z ,.#-]{5,})\"}\n]\n\ndef validate_value(field: Dict[str, Any], value: str) -> float:\n    \"\"\"Heuristic validator that returns a confidence score [0,1].\"\"\"\n    score = 0.5\n    value = (value or \"\").strip()\n    if not value:\n        return 0.0\n    rgx = field.get(\"regex\")\n    if rgx and re.search(rgx, value):\n        score += 0.4\n    if field[\"type\"] == \"date\" and re.search(r\"\\d{2,4}[-/]\", value):\n        score += 0.1\n    return min(1.0, score)\n\n\n# ---------------------------------------\n# 2) PDF Text Extraction (Stdlib only)\n# ---------------------------------------\n\ndef _extract_strings_from_TJ_array(arr: str) -> str:\n    \"\"\"Extract contiguous text from a PDF TJ array operand [ (a) 120 (b) ] TJ (simplified).\"\"\"\n    out = []\n    i, n = 0, len(arr)\n    while i < n:\n        if arr[i] == '(':\n            i += 1\n            buf = []\n            depth = 1\n            esc = False\n            while i < n and depth > 0:\n                c = arr[i]\n                if esc:\n                    buf.append(c)\n                    esc = False\n                else:\n                    if c == '\\\\':\n                        esc = True\n                    elif c == '(':\n                        depth += 1\n                        buf.append(c)\n                    elif c == ')':\n                        depth -= 1\n                        if depth == 0:\n                            break\n                        buf.append(c)\n                    else:\n                        buf.append(c)\n                i += 1\n            out.append(''.join(buf))\n        else:\n            i += 1\n    return ''.join(out)\n\ndef _extract_strings_parens(s: str) -> str:\n    \"\"\"Extract text from tokens like '(Hello) Tj' (handles simple escapes/nesting).\"\"\"\n    out = []\n    i, n = 0, len(s)\n    while i < n:\n        if s[i] == '(':\n            i += 1\n            buf = []\n            depth = 1\n            esc = False\n            while i < n and depth > 0:\n                c = s[i]\n                if esc:\n                    buf.append(c)\n                    esc = False\n                else:\n                    if c == '\\\\':\n                        esc = True\n                    elif c == '(':\n                        depth += 1\n                        buf.append(c)\n                    elif c == ')':\n                        depth -= 1\n                        if depth == 0:\n                            break\n                        buf.append(c)\n                    else:\n                        buf.append(c)\n                i += 1\n            out.append(''.join(buf))\n        else:\n            i += 1\n    return ' '.join(out)\n\ndef _try_inflate(data: bytes) -> bytes:\n    \"\"\"Attempt zlib decompression; return raw bytes if not FlateDecode.\"\"\"\n    try:\n        return zlib.decompress(data)\n    except Exception:\n        return data\n\ndef extract_text_from_pdf_builtin(pdf_path: str) -> str:\n    \"\"\"\n    Best-effort text extraction:\n      - Read raw bytes\n      - Find 'stream ... endstream' content blocks\n      - Inflates with zlib (if possible)\n      - Within inflated content, find BT ... ET blocks\n      - Extract Tj/TJ strings\n      - Normalize whitespace and return a single string\n    \"\"\"\n    with open(pdf_path, 'rb') as f:\n        raw = f.read()\n    text_chunks = []\n    # Find content streams\n    for m in re.finditer(br'stream[\\r\\n]+(.*?)[\\r\\n]+endstream', raw, flags=re.DOTALL):\n        inflated = _try_inflate(m.group(1))\n        try:\n            s = inflated.decode('latin-1', errors='ignore')\n        except Exception:\n            continue\n        # Find text objects\n        for bt in re.finditer(r'BT(.*?)ET', s, flags=re.DOTALL):\n            body = bt.group(1)\n            # ( ... ) Tj\n            for tj in re.finditer(r'\\((?:\\\\.|[^\\)])*\\)\\s*Tj', body):\n                text_chunks.append(_extract_strings_parens(tj.group(0)))\n            # [ ... ] TJ\n            for tja in re.finditer(r'\\[(.*?)\\]\\s*TJ', body, flags=re.DOTALL):\n                text_chunks.append(_extract_strings_from_TJ_array(tja.group(1)))\n    # Fallback if nothing found\n    if not text_chunks:\n        try:\n            s_all = raw.decode('latin-1', errors='ignore')\n            for tj in re.finditer(r'\\((?:\\\\.|[^\\)])*\\)\\s*Tj', s_all):\n                text_chunks.append(_extract_strings_parens(tj.group(0)))\n        except Exception:\n            pass\n    joined = ' '.join(t.strip() for t in text_chunks if t and t.strip())\n    joined = re.sub(r'\\s+', ' ', joined).strip()\n    return joined\n\ndef to_lines(text: str) -> List[str]:\n    \"\"\"Convert long text into pseudo-lines by simple splitting rules (for heuristics).\"\"\"\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return re.split(r'(?<=[\\.:;])\\s+|\\s{2,}', text)\n\n\n# ---------------------------------------\n# 3) Heuristics (alias/regex/proximity)\n# ---------------------------------------\n\ndef match_alias(canonical: str, aliases: List[str], text_header: str, cutoff: float = 0.7) -> bool:\n    \"\"\"True if text_header fuzzily matches canonical name or any alias.\"\"\"\n    candidates = [canonical] + aliases\n    best = difflib.get_close_matches(text_header.lower(), [c.lower() for c in candidates], n=1, cutoff=cutoff)\n    return len(best) > 0\n\ndef regex_extract(page_lines: List[str], field: Dict[str, Any]) -> List[Tuple[str, float, str]]:\n    \"\"\"Find 'Header: value' and next-line patterns; return regex-based candidates.\"\"\"\n    hits = []\n    header_like = [field[\"name\"]] + field[\"aliases\"]\n    header_pat = re.compile(r\"|\".join([re.escape(h) for h in header_like]), re.IGNORECASE)\n    rgx = field.get(\"regex\")\n    for line in page_lines:\n        if header_pat.search(line):\n            after = line.split(\":\", 1)[-1] if \":\" in line else line\n            if rgx:\n                m = re.search(rgx, after)\n                if m:\n                    hits.append((m.group(1).strip(), 0.6, \"regex_header\"))\n    for i, line in enumerate(page_lines):\n        if header_pat.search(line) and i + 1 < len(page_lines):\n            candidate = page_lines[i + 1]\n            if rgx:\n                m = re.search(rgx, candidate)\n                if m:\n                    hits.append((m.group(1).strip(), 0.55, \"regex_nearby\"))\n    return hits\n\ndef proximity_extract(page_lines: List[str], field: Dict[str, Any]) -> List[Tuple[str, float, str]]:\n    \"\"\"When formatting is messy, use alias match + next few tokens as a candidate value.\"\"\"\n    hits = []\n    for line in page_lines:\n        toks = [t.strip(\": \").lower() for t in re.split(r\"[\\s:]+\", line) if t.strip()]\n        for idx, tok in enumerate(toks):\n            if match_alias(field[\"name\"], field[\"aliases\"], tok, cutoff=0.8):\n                tail = toks[idx + 1: idx + 4]\n                if tail:\n                    hits.append((\" \".join(tail), 0.4, \"proximity\"))\n                break\n    return hits\n\n\n# ---------------------------------------\n# 4) LLM Interface (Mock)\n# ---------------------------------------\n\n@dataclass\nclass LLMResult:\n    \"\"\"Container for LLM responses; content is expected to be a JSON string.\"\"\"\n    content: str\n    raw: Dict[str, Any] = field(default_factory=dict)\n\nclass LLMInterface:\n    \"\"\"Swap this out with your real LLM client (OpenAI, Azure, Bedrock, etc.).\"\"\"\n    def extract(self, prompt: str) -> LLMResult:\n        raise NotImplementedError\n\nclass MockLLM(LLMInterface):\n    \"\"\"Offline regex-based 'LLM' that returns JSON for the requested fields.\"\"\"\n    name = \"MockLLM (offline)\"\n    def extract(self, prompt: str) -> LLMResult:\n        m = re.search(r\"SOURCE_TEXT\\n---\\n(.*)\\n---\\n\", prompt, re.DOTALL)\n        src = m.group(1) if m else \"\"\n        data = {}\n        permit = re.search(r\"Permit(?:\\s*Number|\\s*#|\\s*Num|Number|\\s*No\\.?|\\s*ID|\\s*Nbr)[^\\w]*([A-Z0-9-]{3,})\", src, re.IGNORECASE)\n        if permit: data[\"Permit Number\"] = permit.group(1)\n        issue = re.search(r\"(\\d{4}-\\d{2}-\\d{2}|\\d{2}/\\d{2}/\\d{4}|\\d{2}-\\d{2}-\\d{4})\", src)\n        if issue: data[\"Issue Date\"] = issue.group(1)\n        appl = re.search(r\"(?:Applicant|Owner)\\s*[:\\-]?\\s*([A-Za-z ,.'-]{2,})\", src)\n        if appl: data[\"Applicant Name\"] = appl.group(1).strip()\n        addr = re.search(r\"(?:Address|Location)\\s*[:\\-]?\\s*([0-9A-Za-z ,.#-]{5,})\", src)\n        if addr: data[\"Site Address\"] = addr.group(1).strip()\n        return LLMResult(content=json.dumps(data))\n\ndef build_prompt(field_schema: Schema, source_text: str) -> str:\n    \"\"\"Construct a schema-aware prompt asking the LLM to return compact JSON only.\"\"\"\n    schema_desc = [f\"- {f['name']} (type: {f['type']}, aliases: {', '.join(f['aliases'])})\" for f in field_schema]\n    schema_block = \"\\n\".join(schema_desc)\n    return f\"\"\"You are a careful information extraction model. Extract the requested fields.\nReturn only a compact JSON with keys exactly matching canonical field names.\nIf a value is missing, use an empty string.\nSCHEMA\n---\n{schema_block}\n---\nSOURCE_TEXT\n---\n{source_text}\n---\n\"\"\"\n\n\n# ---------------------------------------\n# 5) Ensemble & Pipeline\n# ---------------------------------------\n\ndef clean_tail(value: str) -> str:\n    \"\"\"Trim common spillover tokens from the end of a candidate value (e.g., 'Parcel ID').\"\"\"\n    value = (value or \"\").strip()\n    value = re.split(r\"\\b(Parcel ID|Zoning|Inspector|Contractor License|SCOPE)\\b\", value)[0].strip()\n    value = re.sub(r\"[\\s,:;-]+$\", \"\", value)\n    return value\n\ndef merge_candidates(field: Dict[str, Any], candidates: List[Tuple[str, float, str]]) -> Dict[str, Any]:\n    \"\"\"Deduplicate, score, and choose the best candidate for a field.\"\"\"\n    if not candidates:\n        return {\"value\": \"\", \"score\": 0.0, \"method\": \"\"}\n    uniq, seen = [], set()\n    for v, s, m in candidates:\n        v = clean_tail(v)\n        k = v.strip().lower()\n        if k and k not in seen:\n            uniq.append((v, s, m))\n            seen.add(k)\n    if not uniq:\n        return {\"value\": \"\", \"score\": 0.0, \"method\": \"\"}\n    uniq.sort(key=lambda x: x[1], reverse=True)\n    best_v, best_s, best_m = uniq[0]\n    best_s = max(best_s, validate_value(field, best_v))\n    return {\"value\": best_v, \"score\": round(min(1.0, best_s), 3), \"method\": best_m}\n\ndef ensemble_extract_from_text(text: str, field_schema: Schema, llm: Optional[LLMInterface] = None) -> Tuple[Dict[str, Dict[str, Any]], str]:\n    \"\"\"\n    Run heuristics + LLM on plain text and merge results.\n    Returns (result_dict, llm_name).\n    \"\"\"\n    llm = llm or MockLLM()\n    lines = to_lines(text)\n    per_field_candidates = {f[\"name\"]: [] for f in field_schema}\n    for f in field_schema:\n        per_field_candidates[f[\"name\"]].extend(regex_extract(lines, f))\n        per_field_candidates[f[\"name\"]].extend(proximity_extract(lines, f))\n    prompt = build_prompt(field_schema, text[:20000])\n    try:\n        res = llm.extract(prompt)\n        llm_json = json.loads(res.content) if res and res.content.strip() else {}\n    except Exception:\n        llm_json = {}\n    for f in field_schema:\n        v = (llm_json.get(f[\"name\"], \"\") if isinstance(llm_json, dict) else \"\")\n        if v:\n            per_field_candidates[f[\"name\"]].append((v, 0.7, \"llm\"))\n    merged = {f[\"name\"]: merge_candidates(f, per_field_candidates[f[\"name\"]]) for f in field_schema}\n    return merged, getattr(llm, \"name\", llm.__class__.__name__)\n\ndef extract_fields_builtin(pdf_path: Optional[str] = None, raw_text: Optional[str] = None,\n                           field_schema: Schema = None, llm: Optional[LLMInterface] = None) -> Tuple[Dict[str, Dict[str, Any]], str]:\n    \"\"\"End-to-end entry: parse PDF (or use raw text) and run the ensemble pipeline.\"\"\"\n    assert (pdf_path or raw_text), \"Provide either pdf_path or raw_text\"\n    field_schema = field_schema or FIELD_SCHEMA\n    llm = llm or MockLLM()\n    text = raw_text if raw_text else extract_text_from_pdf_builtin(pdf_path)\n    return ensemble_extract_from_text(text, field_schema, llm)\n\n\n# ---------------------------------------\n# 6) Demo / Example usage\n# ---------------------------------------\n\nPDF_PATH = \"/mnt/data/permit_extra_fields.pdf\"  # change this to your file\nif os.path.exists(PDF_PATH):\n    preview = extract_text_from_pdf_builtin(PDF_PATH)[:300]\n    result, llm_used = extract_fields_builtin(pdf_path=PDF_PATH)\n    payload = {\"llm_used\": llm_used, \"pdf\": os.path.basename(PDF_PATH), \"extracted_fields\": result, \"text_preview_first_300_chars\": preview}\n    with open(\"/mnt/data/extraction_result.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(payload, f, indent=2)\n    print(json.dumps(payload, indent=2))\n    print(\"\\\\nSaved JSON -> /mnt/data/extraction_result.json\")\nelse:\n    # Fallback: quick raw text demo\n    raw = (\n        \"NORTHPORT BUILDING AUTHORITY\\\\n\"\n        \"Permit # : ZZZ-2025-42\\\\n\"\n        \"Owner : North Shore Homes\\\\n\"\n        \"Date of Issue - 15-08-2025\\\\n\"\n        \"Location: 1600 Lake View Rd, Northport, CA 96001\\\\n\"\n    )\n    result, llm_used = ensemble_extract_from_text(raw, FIELD_SCHEMA)\n    print(\"LLM used:\", llm_used)\n    print(json.dumps(result, indent=2))\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}